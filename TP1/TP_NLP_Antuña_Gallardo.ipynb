{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "!pip install deep-translator\n",
    "!pip install imbalanced-learn\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install scikit-learn\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Franco-\n",
      "[nltk_data]     SEC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from deep_translator import GoogleTranslator\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "import gdown\n",
    "from IPython.display import display\n",
    "import time\n",
    "import sys\n",
    "from IPython.display import clear_output\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    TextPreprocessor es una clase de preprocesamiento de texto para limpiar datos textuales.\n",
    "    Hereda de BaseEstimator y TransformerMixin de scikit-learn para integrarse en pipelines\n",
    "    y garantizar compatibilidad con la API de scikit-learn.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Conjunto de stopwords en ingl칠s que ser치n eliminadas del texto\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Limpia el texto de entrada realizando los siguientes pasos:\n",
    "        1. Convierte el texto a min칰sculas.\n",
    "        2. Elimina la puntuaci칩n.\n",
    "        3. Elimina las stopwords.\n",
    "        \n",
    "        Par치metros:\n",
    "        text (str): Texto de entrada a limpiar.\n",
    "\n",
    "        Retorna:\n",
    "        str: Texto limpio.\n",
    "        \"\"\"\n",
    "        # Convertir el texto a min칰sculas\n",
    "        text = text.lower()\n",
    "        # Eliminar puntuaci칩n del texto\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        # Eliminar stopwords filtrando palabras en el texto\n",
    "        text = ' '.join(word for word in text.split() if word not in self.stop_words)\n",
    "        return text\n",
    "    \n",
    "    def fit(self, X: pd.Series, y=None) -> \"TextPreprocessor\":\n",
    "        \"\"\"\n",
    "        M칠todo de ajuste para cumplir con la API de scikit-learn.\n",
    "        Este transformador no necesita aprender nada de los datos.\n",
    "        \n",
    "        Par치metros:\n",
    "        X (pd.Series): Serie de pandas con los datos textuales de entrada.\n",
    "        y: Par치metro opcional para compatibilidad; no se utiliza.\n",
    "\n",
    "        Retorna:\n",
    "        TextPreprocessor: Retorna la instancia actual del objeto.\n",
    "        \"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.Series) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Aplica el proceso de limpieza de texto a los datos de entrada.\n",
    "        \n",
    "        Par치metros:\n",
    "        X (pd.Series o str): Datos de entrada a transformar, ya sea una Serie de pandas o una cadena individual.\n",
    "        \n",
    "        Retorna:\n",
    "        pd.Series o str: Datos limpiados, retornados como una Serie de pandas si la entrada fue una Serie,\n",
    "                         o como una cadena individual si la entrada fue una cadena.\n",
    "        \n",
    "        Excepciones:\n",
    "        ValueError: Si el tipo de entrada no es ni una Serie de pandas ni una cadena.\n",
    "        \"\"\"\n",
    "        # Verificar si la entrada es una Serie de pandas\n",
    "        if isinstance(X, pd.Series):\n",
    "            return X.apply(self.clean_text)\n",
    "        # Verificar si la entrada es una cadena individual\n",
    "        elif isinstance(X, str):\n",
    "            return self.clean_text(X)\n",
    "        # Lanzar un error si el tipo de entrada es inv치lido\n",
    "        else:\n",
    "            raise ValueError(\"La entrada debe ser una cadena o una Serie de pandas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_database() -> None:\n",
    "    \"\"\"\n",
    "    Descarga varios archivos de datos desde Google Drive y los guarda en el directorio 'database'.\n",
    "    Cada archivo se descarga mediante su ID de Google Drive espec칤fico y se guarda con un nombre predefinido.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Diccionario que contiene el ID de cada archivo y su ruta de salida correspondiente\n",
    "    files_to_download = {\n",
    "        '1xkN6-OKnp8XOCjcRXFHyFAmKN6GODpji': 'database/bgg_database.csv',\n",
    "        '1b4PUV-SRkUm7A_vLeRZMkA80rsABJjuV': 'database/IMDB-Movie-Data.csv',\n",
    "        '1zJYm3kKzy1HQzta6aCmikTENgPOIrx28': 'database/libros.csv',\n",
    "        '1dfduFDeHbIFoXhNj8FCV7yvuvwKn_rpl': 'database/negative-words.txt',\n",
    "        '17jI37fqKDp9yzgAiwTIE3UmFvkL5XHO1': 'database/positive-words.txt'\n",
    "    }\n",
    "\n",
    "    # Descargar cada archivo usando su ID y guardarlo en la ubicaci칩n especificada\n",
    "    for file_id, output_path in files_to_download.items():\n",
    "        # Construir la URL de descarga usando el ID del archivo\n",
    "        download_url = f'https://drive.google.com/uc?id={file_id}'\n",
    "        # Descargar el archivo desde Google Drive y guardarlo en la ruta especificada\n",
    "        gdown.download(download_url, output_path, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_books(developer_mode: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Descarga informaci칩n de libros populares del sitio Proyecto Gutenberg.\n",
    "    Extrae el t칤tulo, autor, enlace, resumen y temas de cada libro, y guarda los datos en un archivo CSV.\n",
    "\n",
    "    Par치metros:\n",
    "    developer_mode (bool): Modo desarrollador. Si es True, imprime informaci칩n detallada de cada libro mientras se procesa.\n",
    "\n",
    "    Salida:\n",
    "    None: La funci칩n guarda los datos en un archivo CSV y no retorna ning칰n valor.\n",
    "    \"\"\"\n",
    "    \n",
    "    # URL de la p치gina de libros m치s populares\n",
    "    url: str = \"https://www.gutenberg.org/browse/scores/top1000.php#books-last1\"\n",
    "\n",
    "    # Realizar solicitud a la p치gina principal de libros\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Seleccionar enlaces de libros que contienen '/ebooks/' seguido de un n칰mero (ID de libro)\n",
    "    libros = soup.select(\"li > a[href^='/ebooks/']\")\n",
    "    datos_libros: List[Dict[str, str]] = []  # Lista para almacenar la informaci칩n de cada libro\n",
    "\n",
    "    # Procesar cada enlace de libro\n",
    "    for libro in libros:\n",
    "        texto_completo: str = libro.get_text()\n",
    "        enlace_anidado: str = libro['href']\n",
    "        \n",
    "        # Verificar que el enlace tenga un n칰mero despu칠s de \"/ebooks/\"\n",
    "        if enlace_anidado.split('/ebooks/')[-1].isdigit():\n",
    "            # Separar t칤tulo y autor; si no hay autor, se asigna \"Desconocido\"\n",
    "            titulo_y_autor: List[str] = texto_completo.split(\" by \", 1)\n",
    "            titulo: str = titulo_y_autor[0].strip()\n",
    "            autor: str = titulo_y_autor[1].strip() if len(titulo_y_autor) > 1 else \"Desconocido\"\n",
    "            \n",
    "            # Construir URL completa para la p치gina del libro\n",
    "            url_libro: str = f\"https://www.gutenberg.org{enlace_anidado}\"\n",
    "            \n",
    "            # Realizar solicitud a la p치gina individual del libro\n",
    "            response_libro = requests.get(url_libro)\n",
    "            response_libro.raise_for_status()\n",
    "            soup_libro = BeautifulSoup(response_libro.text, 'html.parser')\n",
    "            \n",
    "            # Intentar extraer el resumen si est치 disponible\n",
    "            summary: str = \"\"\n",
    "            summary_row = soup_libro.find('th', text='Summary')\n",
    "            if summary_row:\n",
    "                summary_td = summary_row.find_next(\"td\")\n",
    "                if summary_td:\n",
    "                    summary = summary_td.get_text(strip=True)\n",
    "            \n",
    "            # Extraer temas (subjects) del libro\n",
    "            subjects: List[str] = []\n",
    "            subject_rows = soup_libro.find_all('th', text='Subject')\n",
    "            for subject_row in subject_rows:\n",
    "                subject_td = subject_row.find_next(\"td\")\n",
    "                if subject_td:\n",
    "                    subject_links = subject_td.find_all('a')\n",
    "                    for link in subject_links:\n",
    "                        subjects.append(link.get_text(strip=True))\n",
    "\n",
    "            # Combinar los temas en una sola cadena\n",
    "            subjects_combined: str = \", \".join(subjects)\n",
    "\n",
    "            # A침adir la informaci칩n del libro a la lista de datos\n",
    "            datos_libros.append({\n",
    "                \"T칤tulo\": titulo,\n",
    "                \"Autor\": autor,\n",
    "                \"Enlace\": url_libro,\n",
    "                \"Resumen\": summary,\n",
    "                \"Subjects\": subjects_combined\n",
    "            })\n",
    "            \n",
    "            # Imprimir informaci칩n detallada en modo desarrollador\n",
    "            if developer_mode:\n",
    "                print(\"T칤tulo:\", titulo, \"Autor:\", autor, \"URL:\", url_libro, \"Resumen:\", summary, \"Temas:\", subjects_combined)\n",
    "\n",
    "    # Crear un DataFrame y guardar los datos en un archivo CSV\n",
    "    df_libros = pd.DataFrame(datos_libros)\n",
    "    df_libros.to_csv('libros.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_words(file_path: str) -> set[str]:\n",
    "    \"\"\"\n",
    "    Carga palabras desde un archivo y las retorna como un conjunto de cadenas.\n",
    "    \n",
    "    Par치metros:\n",
    "    file_path (str): La ruta del archivo de texto que contiene una palabra por l칤nea.\n",
    "    \n",
    "    Retorna:\n",
    "    Set[str]: Un conjunto de palabras (strings) obtenidas del archivo, sin espacios en blanco.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Lee todas las l칤neas del archivo y elimina los espacios en blanco alrededor de cada palabra\n",
    "        words = {line.strip() for line in file.readlines()}\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets_union() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Carga y preprocesa tres conjuntos de datos (pel칤culas, juegos de mesa y libros),\n",
    "    limpiando el texto de las columnas relevantes y combin치ndolos en un solo DataFrame.\n",
    "    \n",
    "    Realiza las siguientes acciones:\n",
    "    1. Procesa y limpia los datos de pel칤culas.\n",
    "    2. Procesa y limpia los datos de juegos de mesa.\n",
    "    3. Procesa y limpia los datos de libros.\n",
    "    4. Combina todos los datos en un solo DataFrame.\n",
    "\n",
    "    Retorna:\n",
    "    pd.DataFrame: Un DataFrame combinado que contiene los datos preprocesados.\n",
    "    \"\"\"\n",
    "    # Instanciar el preprocesador de texto\n",
    "    text_preprocessor = TextPreprocessor()\n",
    "\n",
    "    # 1. Procesamiento de Pel칤culas\n",
    "    peliculas_dataframe = pd.read_csv('database/IMDB-Movie-Data.csv')\n",
    "    peliculas_dataframe['category'] = 'pelicula'\n",
    "    peliculas_dataframe['text'] = (\n",
    "        peliculas_dataframe['Title'].fillna('').apply(text_preprocessor.transform) + \" \" +\n",
    "        peliculas_dataframe['Genre'].fillna('').apply(text_preprocessor.transform) + \" \" +\n",
    "        peliculas_dataframe['Description'].fillna('').apply(text_preprocessor.transform) + \" \" +\n",
    "        peliculas_dataframe['Director'].fillna('').apply(text_preprocessor.transform) + \" \" +\n",
    "        peliculas_dataframe['Actors'].fillna('').apply(text_preprocessor.transform)\n",
    "    )\n",
    "    peliculas_dataframe.rename(columns={'Title': 'Titulo', 'Director': 'Autor', 'Description': 'Resumen', 'Genre': 'Subjects'}, inplace=True)\n",
    "\n",
    "    # 2. Procesamiento de Juegos de Mesa\n",
    "    juegos_mesa_dataframe = pd.read_csv('database/bgg_database.csv')\n",
    "    juegos_mesa_dataframe['category'] = 'juego'\n",
    "    juegos_mesa_dataframe['text'] = (\n",
    "        juegos_mesa_dataframe['game_name'].fillna('').apply(text_preprocessor.transform) + \" \" +\n",
    "        juegos_mesa_dataframe['description'].fillna('').apply(text_preprocessor.transform) + \" \" +\n",
    "        juegos_mesa_dataframe['designers'].apply(lambda x: ' '.join(eval(x)) if isinstance(x, str) else '').apply(text_preprocessor.transform).fillna('') + \" \" +\n",
    "        juegos_mesa_dataframe['mechanics'].apply(lambda x: ' '.join(eval(x)) if isinstance(x, str) else '').apply(text_preprocessor.transform).fillna('') + \" \" +\n",
    "        juegos_mesa_dataframe['categories'].apply(lambda x: ' '.join(eval(x)) if isinstance(x, str) else '').apply(text_preprocessor.transform).fillna('')\n",
    "    )\n",
    "    juegos_mesa_dataframe.rename(columns={'game_name': 'Titulo','game_href': 'Enlace', 'designers': 'Autor', 'description': 'Resumen', 'mechanics': 'Subjects'}, inplace=True)\n",
    "\n",
    "    # 3. Procesamiento de Libros\n",
    "    libros_dataframe = pd.read_csv('database/libros.csv')\n",
    "    libros_dataframe['category'] = 'libro'\n",
    "    libros_dataframe['text'] = (\n",
    "        libros_dataframe['T칤tulo'].fillna('').apply(text_preprocessor.transform) + \" \" +\n",
    "        libros_dataframe['Resumen'].fillna('').apply(text_preprocessor.transform) + \" \" +\n",
    "        libros_dataframe['Subjects'].fillna('').apply(text_preprocessor.transform) + \" \" +\n",
    "        libros_dataframe['Autor'].fillna('').apply(text_preprocessor.transform)\n",
    "    )\n",
    "    libros_dataframe.rename(columns={'T칤tulo': 'Titulo', 'Autor': 'Autor', 'Resumen': 'Resumen', 'Subjects': 'Subjects'}, inplace=True)\n",
    "\n",
    "    # Concatenar los DataFrames de pel칤culas, juegos de mesa y libros en un solo DataFrame\n",
    "    dataframe_bbdd = pd.concat(\n",
    "        [peliculas_dataframe[['Titulo', 'Autor', 'Resumen', 'Subjects', 'category', 'text']],\n",
    "         juegos_mesa_dataframe[['Titulo', 'Enlace','Autor', 'Resumen', 'Subjects', 'category', 'text']],\n",
    "         libros_dataframe[['Titulo', 'Enlace','Autor', 'Resumen', 'Subjects', 'category', 'text']]\n",
    "        ],\n",
    "        ignore_index=True  # Para reiniciar el 칤ndice despu칠s de la concatenaci칩n\n",
    "    )\n",
    "\n",
    "    return dataframe_bbdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text: str, positive_words: set[str], negative_words: set[str]) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Cuenta las palabras positivas y negativas en un texto dado.\n",
    "\n",
    "    Esta funci칩n cuenta cu치ntas palabras del texto pertenecen al conjunto de\n",
    "    palabras positivas y cu치ntas al conjunto de palabras negativas.\n",
    "\n",
    "    Par치metros:\n",
    "    - text (str): El texto en el cual se realizar치n las b칰squedas de palabras.\n",
    "    - positive_words (Set[str]): Un conjunto de palabras consideradas positivas.\n",
    "    - negative_words (Set[str]): Un conjunto de palabras consideradas negativas.\n",
    "\n",
    "    Retorna:\n",
    "    - pd.Series: Una serie de pandas con dos valores, el conteo de palabras positivas\n",
    "      en el texto y el conteo de palabras negativas en el texto, en ese orden.\n",
    "    \"\"\"\n",
    "    # Contar las palabras positivas en el texto\n",
    "    positive_count = sum(1 for word in text.split() if word in positive_words)\n",
    "    \n",
    "    # Contar las palabras negativas en el texto\n",
    "    negative_count = sum(1 for word in text.split() if word in negative_words)\n",
    "    \n",
    "    # Retornar los resultados como una Serie de pandas\n",
    "    return pd.Series([positive_count, negative_count], index=['Positive_Count', 'Negative_Count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_sentiments(developer_mode: bool) -> tuple[LogisticRegression, set[str], set[str]]:\n",
    "    \"\"\"\n",
    "    Entrena un modelo de regresi칩n log칤stica para predecir sentimientos (positivo o negativo) basado en palabras clave positivas y negativas.\n",
    "\n",
    "    La funci칩n carga un conjunto de palabras positivas y negativas desde archivos, las convierte en un DataFrame y genera\n",
    "    caracter칤sticas basadas en la frecuencia de aparici칩n de palabras positivas y negativas en el texto. Luego entrena un modelo\n",
    "    de regresi칩n log칤stica para clasificar las palabras como \"feliz\" (positivo) o \"triste\" (negativo).\n",
    "\n",
    "    Par치metros:\n",
    "    - developer_mode (bool): Un indicador para mostrar las m칠tricas de evaluaci칩n del modelo (accuracy, matriz de confusi칩n, etc.).\n",
    "\n",
    "    Retorna:\n",
    "    - model (LogisticRegression): El modelo entrenado de regresi칩n log칤stica.\n",
    "    - positive_words (Set[str]): El conjunto de palabras positivas cargadas.\n",
    "    - negative_words (Set[str]): El conjunto de palabras negativas cargadas.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cargar palabras positivas y negativas\n",
    "    positive_words = load_words('database/positive-words.txt')\n",
    "    negative_words = load_words('database/negative-words.txt')\n",
    "    \n",
    "    # Crear un conjunto de datos basado en las palabras\n",
    "    data = {\n",
    "        'text': [],\n",
    "        'label': []\n",
    "    }\n",
    "\n",
    "    # Llenar el conjunto de datos con palabras positivas y negativas\n",
    "    for word in positive_words:\n",
    "        data['text'].append(word)\n",
    "        data['label'].append('feliz')  # Etiqueta \"feliz\" para palabras positivas\n",
    "\n",
    "    for word in negative_words:\n",
    "        data['text'].append(word)\n",
    "        data['label'].append('triste')  # Etiqueta \"triste\" para palabras negativas\n",
    "\n",
    "    # Crear un DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Crear caracter칤sticas\n",
    "    df[['positive_count', 'negative_count']] = df['text'].apply(lambda x: count_words(x, positive_words, negative_words))\n",
    "\n",
    "    # Convertir etiquetas a n칰meros: \"feliz\" -> 1, \"triste\" -> 0\n",
    "    df['label'] = df['label'].map({'feliz': 1, 'triste': 0})\n",
    "\n",
    "    # Separar caracter칤sticas y etiquetas\n",
    "    X = df[['positive_count', 'negative_count']]  # Caracter칤sticas\n",
    "    y = df['label']  # Etiquetas\n",
    "\n",
    "    # Dividir el conjunto de datos en entrenamiento y prueba (60% entrenamiento, 40% prueba)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "    # Entrenar el modelo de regresi칩n log칤stica\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    if developer_mode:\n",
    "        # Predecir en el conjunto de prueba\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Evaluar el modelo\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        confusion = confusion_matrix(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred)\n",
    "\n",
    "        print(f'Accuracy: {accuracy}')\n",
    "        print('Confusion Matrix:')\n",
    "        print(confusion)\n",
    "        print('Classification Report:')\n",
    "        print(report)\n",
    "    \n",
    "    # Retornar el modelo entrenado y los conjuntos de palabras positivas y negativas\n",
    "    return model, positive_words, negative_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataset: pd.DataFrame, developer_mode: bool) -> any:\n",
    "    \"\"\"\n",
    "    Entrena un modelo de clasificaci칩n de texto utilizando regresi칩n log칤stica y TF-IDF para vectorizaci칩n de texto.\n",
    "    \n",
    "    La funci칩n divide el conjunto de datos en entrenamiento y prueba, luego entrena un modelo con regresi칩n log칤stica y realiza \n",
    "    una validaci칩n cruzada para evaluar el rendimiento del modelo. Si 'developer_mode' es True, imprime las puntuaciones de la \n",
    "    validaci칩n cruzada y la puntuaci칩n media.\n",
    "    \n",
    "    Par치metros:\n",
    "    - dataset (pd.DataFrame): Un DataFrame con columnas 'text' (texto) y 'category' (categor칤as de las clases).\n",
    "    - developer_mode (bool): Un indicador para imprimir las m칠tricas de la validaci칩n cruzada si es True.\n",
    "\n",
    "    Retorna:\n",
    "    - model (Pipeline): El modelo entrenado con regresi칩n log칤stica y vectorizaci칩n TF-IDF.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Separar el dataset en conjunto de entrenamiento y conjunto de prueba (80% - 20%)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset['text'], dataset['category'], test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Crear un pipeline con TF-IDF para la vectorizaci칩n de texto y regresi칩n log칤stica\n",
    "    model = make_pipeline(\n",
    "        TfidfVectorizer(),  # Vectoriza el texto con TF-IDF\n",
    "        LogisticRegression(C=1.0, max_iter=200, solver='liblinear')  # Modelo de regresi칩n log칤stica\n",
    "    )\n",
    "    \n",
    "    # Entrenar el modelo\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Realizar validaci칩n cruzada (5 pliegues)\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5)  # 5-fold cross-validation\n",
    "    \n",
    "    # Si 'developer_mode' es True, imprimir detalles de la validaci칩n cruzada\n",
    "    if developer_mode:\n",
    "        print(f\"Cross-validation scores: {cv_scores}\")\n",
    "        print(f\"Mean cross-validation score: {cv_scores.mean()}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_recommender(dataset) -> any:\n",
    "    \"\"\"\n",
    "    Esta funci칩n vectoriza los textos del dataset utilizando el algoritmo TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "    para representar los textos como vectores num칠ricos.\n",
    "\n",
    "    Par치metros:\n",
    "    - dataset (pd.DataFrame): El DataFrame que contiene la columna 'text', que contiene los textos a vectorizar.\n",
    "\n",
    "    Retorna:\n",
    "    - tfidf_matrix (scipy.sparse.csr_matrix): Matriz dispersa de caracter칤sticas vectorizadas del texto.\n",
    "    - tfidf_vectorizer (TfidfVectorizer): El objeto del vectorizador que puede usarse para transformar m치s texto.\n",
    "    \"\"\"\n",
    "    # Crear un objeto TfidfVectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Ajustar y transformar los textos en una matriz TF-IDF\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(dataset['text'])\n",
    "    \n",
    "    return tfidf_matrix, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_entrada_usuario(texto) -> str:\n",
    "    \"\"\"\n",
    "    Procesa la entrada del usuario de la siguiente manera:\n",
    "    1. Traduce el texto de espa침ol a ingl칠s.\n",
    "    2. Preprocesa el texto traducido para limpieza (ej. eliminaci칩n de stopwords, caracteres no deseados).\n",
    "\n",
    "    Par치metros:\n",
    "    - texto (str): Texto en espa침ol ingresado por el usuario.\n",
    "\n",
    "    Retorna:\n",
    "    - texto_procesado (str): Texto limpio y procesado en ingl칠s.\n",
    "    \"\"\"\n",
    "    # Traducir al ingl칠s\n",
    "    texto_traducido = GoogleTranslator(source='es', target='en').translate(texto)\n",
    "\n",
    "    # Preprocesar el texto traducido (limpieza de texto)\n",
    "    texto_procesado = TextPreprocessor().clean_text(texto_traducido)\n",
    "\n",
    "    return texto_procesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_item(prompt, model) -> str:\n",
    "    \"\"\"\n",
    "    Esta funci칩n recomienda un art칤culo basado en el prompt del usuario.\n",
    "    El modelo predictivo se usa para predecir la categor칤a del texto de entrada.\n",
    "    \n",
    "    Par치metros:\n",
    "    - prompt (str): El texto de entrada del usuario.\n",
    "    - model (modelo entrenado): El modelo que se usar치 para predecir la categor칤a.\n",
    "\n",
    "    Retorna:\n",
    "    - category (str): La categor칤a predicha por el modelo para el prompt ingresado.\n",
    "    \"\"\"\n",
    "    # Predecir la categor칤a del prompt\n",
    "    category = model.predict([prompt])[0]\n",
    "    \n",
    "    return category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiments(model, positive_words, negative_words, frase, developer_mode) -> str:\n",
    "    \"\"\"\n",
    "    Esta funci칩n predice la emoci칩n (feliz o triste) en funci칩n de la frase dada.\n",
    "    La predicci칩n se realiza contando las palabras positivas y negativas en la frase procesada \n",
    "    y utilizando un modelo previamente entrenado.\n",
    "\n",
    "    Par치metros:\n",
    "    - model (LogisticRegression o similar): El modelo entrenado para predecir la emoci칩n.\n",
    "    - positive_words (list): Lista de palabras positivas.\n",
    "    - negative_words (list): Lista de palabras negativas.\n",
    "    - frase (str): La entrada de texto a procesar y analizar.\n",
    "    - developer_mode (bool): Indica si se deben imprimir detalles adicionales para depuraci칩n.\n",
    "\n",
    "    Retorna:\n",
    "    - emocion (str): La emoci칩n predicha, que puede ser 'feliz' o 'triste'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Procesar la entrada de texto (traducci칩n y limpieza)\n",
    "    entrada_procesada = procesar_entrada_usuario(frase)\n",
    "    \n",
    "    if developer_mode:\n",
    "        print(\"Texto procesado:\", entrada_procesada)\n",
    "\n",
    "    # Contar las palabras positivas y negativas en la entrada procesada\n",
    "    entrada_counts = count_words(entrada_procesada, positive_words, negative_words)\n",
    "\n",
    "    # Asegurarse de que la entrada est칠 en formato 2D para la predicci칩n\n",
    "    entrada_vectorizada = np.array([entrada_counts]).reshape(1, -1)\n",
    "\n",
    "    # Obtener la predicci칩n del modelo: 1 para 'feliz', 0 para 'triste'\n",
    "    prediccion = model.predict(entrada_vectorizada)[0]\n",
    "\n",
    "    # Determinar la emoci칩n basada en la predicci칩n (1 -> feliz, 0 -> triste)\n",
    "    emocion = 'feliz' if prediccion == 1 else 'triste'\n",
    "    \n",
    "    return emocion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_recomendation(prompt, category, model, dataframe_bbdd) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Esta funci칩n genera recomendaciones basadas en un 'prompt' de entrada,\n",
    "    filtrando las recomendaciones seg칰n la categor칤a dada, y luego calculando\n",
    "    la similitud de coseno entre el 'prompt' y los elementos del conjunto de datos.\n",
    "\n",
    "    Par치metros:\n",
    "    - prompt (str): El texto de entrada para la recomendaci칩n.\n",
    "    - category (str): La categor칤a de recomendaciones a considerar.\n",
    "    - model (Pipeline): El modelo entrenado que incluye el vectorizador TF-IDF.\n",
    "    - dataframe_bbdd (DataFrame): El conjunto de datos de recomendaciones, con columnas 'category', 'text', 'Titulo', 'Enlace'.\n",
    "    \n",
    "    Retorna:\n",
    "    - Titulo (str): El t칤tulo de la recomendaci칩n m치s similar al 'prompt'.\n",
    "    - Enlace (str): El enlace relacionado con la recomendaci칩n.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filtrar el dataframe_bbdd para obtener solo las recomendaciones de la categor칤a dada.\n",
    "    recommendations = dataframe_bbdd[dataframe_bbdd['category'] == category].copy().reset_index(drop=True)\n",
    "\n",
    "    # Si no hay recomendaciones disponibles para la categor칤a, devolver un mensaje.\n",
    "    if recommendations.empty:\n",
    "        return \"No hay recomendaciones disponibles\", \"\"\n",
    "\n",
    "    # Vectorizar el 'prompt' de recomendaci칩n utilizando el vectorizador TF-IDF del modelo.\n",
    "    prompt_vector = model.named_steps['tfidfvectorizer'].transform([prompt])\n",
    "\n",
    "    # Vectorizar el texto de las recomendaciones del dataset usando el vectorizador TF-IDF del modelo.\n",
    "    dataset_vector = model.named_steps['tfidfvectorizer'].transform(recommendations['text'])\n",
    "\n",
    "    # Calcular la similitud de coseno entre el 'prompt' y el texto de las recomendaciones del dataset.\n",
    "    similarity_scores = cosine_similarity(prompt_vector, dataset_vector)\n",
    "\n",
    "    # Obtener el 칤ndice del 칤tem m치s similar en el dataset basado en la similitud m치s alta.\n",
    "    most_similar_idx = similarity_scores.argmax()\n",
    "\n",
    "    # Obtener la recomendaci칩n correspondiente del dataframe_bbdd utilizando el 칤ndice de la similitud m치s alta.\n",
    "    recommended_item = recommendations.iloc[most_similar_idx]\n",
    "\n",
    "    # Devolver el t칤tulo y el enlace de la recomendaci칩n m치s similar.\n",
    "    return recommended_item['Titulo'], recommended_item['Enlace']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_user():\n",
    "    \"\"\"\n",
    "    Esta funci칩n solicita al usuario una entrada con dos partes: \n",
    "    un sentimiento y una recomendaci칩n, separados por una coma. \n",
    "    Si el usuario escribe \"chau\", la funci칩n termina la interacci칩n.\n",
    "    \n",
    "    Retorna:\n",
    "    - prompt_sentimiento (str): El texto procesado relacionado con el sentimiento.\n",
    "    - prompt_recomendacion (str): El texto procesado relacionado con la recomendaci칩n, si existe.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Solicitar la entrada del usuario\n",
    "    prompt = input(\"쮺칩mo est치s hoy? 쯈u칠 quieres hacer? Puedes despedirte diciendo 'Chau': \")\n",
    "    \n",
    "    # Verificar si el usuario quiere terminar la interacci칩n\n",
    "    if prompt.lower() == \"chau\":\n",
    "        return \"exit\", \"exit\"\n",
    "    \n",
    "    # Separar el prompt en sentimientos y recomendaciones usando la coma como delimitador\n",
    "    prompts = prompt.split(\",\")\n",
    "    \n",
    "    # Limpiar y asignar los textos a sus respectivas variables\n",
    "    sentiment_prompt = prompts[0].strip()  # El texto antes de la coma se considera el sentimiento\n",
    "    recommendation_prompt = prompts[1].strip() if len(prompts) > 1 else \"\"  # El texto despu칠s de la coma se considera la recomendaci칩n, si existe\n",
    "\n",
    "    # Traducir los prompts de espa침ol a ingl칠s usando Google Translator\n",
    "    translated_sentiment = GoogleTranslator(source='es', target='en').translate(sentiment_prompt)\n",
    "    translated_recommendation = GoogleTranslator(source='es', target='en').translate(recommendation_prompt)\n",
    "\n",
    "    # Procesar el texto para eliminar caracteres no deseados y hacer limpieza\n",
    "    prompt_sentimiento = TextPreprocessor().clean_text(translated_sentiment)\n",
    "    prompt_recomendacion = TextPreprocessor().clean_text(translated_recommendation)\n",
    "\n",
    "    # Retornar los prompts procesados\n",
    "    return prompt_sentimiento, prompt_recomendacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_slow(text, delay=0.04):\n",
    "    \"\"\"\n",
    "    Imprime un texto lentamente, mostrando un car치cter a la vez con un retraso especificado.\n",
    "    \n",
    "    Par치metros:\n",
    "    - text (str): El texto que se imprimir치 lentamente.\n",
    "    - delay (float, opcional): El tiempo de espera entre cada car치cter en segundos. \n",
    "      Por defecto es 0.04 segundos.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Recorrer cada car치cter en el texto\n",
    "    for char in text:\n",
    "        sys.stdout.write(char)  # Escribir el car치cter en la salida est치ndar\n",
    "        sys.stdout.flush()  # Asegurarse de que el car치cter se imprima inmediatamente\n",
    "        time.sleep(delay)  # Esperar antes de imprimir el siguiente car치cter\n",
    "    \n",
    "    # Imprimir una nueva l칤nea al final\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(developer_mode) -> None:\n",
    "    \"\"\"\n",
    "    Funci칩n principal que gestiona el flujo de trabajo de la aplicaci칩n.\n",
    "    Verifica la existencia de archivos necesarios, entrena el modelo y\n",
    "    maneja la interacci칩n con el usuario para predecir sentimientos y \n",
    "    hacer recomendaciones.\n",
    "\n",
    "    Par치metros:\n",
    "    - developer_mode (bool): Si est치 activado, muestra informaci칩n detallada de depuraci칩n.\n",
    "    \"\"\"\n",
    "\n",
    "    # Verificar la existencia de los archivos necesarios\n",
    "    required_files = [\n",
    "        'database/bgg_database.csv',\n",
    "        'database/IMDB-Movie-Data.csv',\n",
    "        'database/libros.csv',\n",
    "        'positive-words.txt',\n",
    "        'negative-words.txt'\n",
    "    ]\n",
    "\n",
    "    # Comprobar si los archivos existen, si no, crear y cargar seg칰n corresponda\n",
    "    if not all(os.path.exists(file) for file in required_files):\n",
    "        # Crear el directorio 'database' si no existe\n",
    "        if not os.path.exists('database'):\n",
    "            os.makedirs('database')\n",
    "\n",
    "        # Si el archivo de libros no existe, cargarlo\n",
    "        if not os.path.exists('database/libros.csv'):\n",
    "            load_books()\n",
    "\n",
    "        # Cargar las bases de datos necesarias\n",
    "        load_database()\n",
    "\n",
    "    # Entrenar el modelo de sentimientos\n",
    "    model_sents, positive_words, negative_words = model_sentiments(developer_mode)\n",
    "\n",
    "    # Cargar el dataframe de la base de datos combinada\n",
    "    dataframe_bbdd = load_datasets_union()\n",
    "\n",
    "    # Entrenar el modelo de recomendaciones\n",
    "    recommendation_model = train_model(dataframe_bbdd, developer_mode)\n",
    "\n",
    "    # Ciclo principal de interacci칩n con el usuario\n",
    "    while True:\n",
    "        # Obtener la entrada del usuario (sentimiento y recomendaci칩n)\n",
    "        prompt_sentimiento, prompt_recomendacion = input_user()\n",
    "\n",
    "        # Salir si el usuario escribe 'chau'\n",
    "        if prompt_sentimiento == \"exit\":\n",
    "            break\n",
    "\n",
    "        # Predecir sentimiento\n",
    "        sentimiento = predict_sentiments(model_sents, positive_words, negative_words, prompt_sentimiento, developer_mode)\n",
    "\n",
    "        # Predecir la categor칤a para la recomendaci칩n\n",
    "        recommended_category = recommend_item(prompt_recomendacion, recommendation_model)\n",
    "\n",
    "        if developer_mode:\n",
    "            # Imprimir informaci칩n detallada de la depuraci칩n\n",
    "            print(\"Prompt sentimiento:\", prompt_sentimiento)\n",
    "            print(\"Prompt recomendaci칩n:\", prompt_recomendacion)\n",
    "            print(\"Categor칤a recomendada:\", recommended_category)\n",
    "\n",
    "        # Obtener la recomendaci칩n final (t칤tulo y enlace)\n",
    "        recommended_title, recommended_link = predict_recomendation(prompt_recomendacion, recommended_category, recommendation_model, dataframe_bbdd)\n",
    "\n",
    "        # Mostrar los resultados con efectos visuales\n",
    "        print_slow(f\"Entiendo que hoy est치s {sentimiento}\")\n",
    "        print_slow(f\"Mi recomendaci칩n es: {recommended_title} y el enlace es: {recommended_link} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoy fue un buen dia, quiero ver una pelicula de criminales intergalacticos\n",
    "Tenemos un dia increible con amigos, queremos un juego de mesa de aventuras y ciencia ficcion\n",
    "Hoy fue un dia horrible, quiero leer un libro de biologia\n",
    "Hoy me encuentro mal, quiero ver una pelicula de comedia\n",
    "Hoy discuti con un amigo, me gustaria algo divertido para estar solo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot() -> None:\n",
    "    \"\"\"\n",
    "    Funci칩n principal que interact칰a con el usuario, configura el modo de desarrollador,\n",
    "    muestra un mensaje de bienvenida con instrucciones, y luego llama a la funci칩n principal\n",
    "    que gestiona la predicci칩n de sentimientos y recomendaciones.\n",
    "    \"\"\"\n",
    "\n",
    "    # Activar el modo desarrollador seg칰n la respuesta del usuario\n",
    "    developer_mode = False\n",
    "    warnings.filterwarnings(\"ignore\")  # Ignorar advertencias durante la ejecuci칩n\n",
    "\n",
    "    # Preguntar al usuario si desea activar el modo desarrollador\n",
    "    if input(\"쮻esea activar el modo desarrollador? (s/n): \\n\") == \"s\":\n",
    "        developer_mode = True\n",
    "\n",
    "    # Limpiar la salida de consola antes de mostrar los mensajes de bienvenida\n",
    "    clear_output(wait=False)\n",
    "\n",
    "    # Mostrar mensajes de bienvenida y reglas de interacci칩n\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print_slow(\"Hola, mi nombre es ChatLPJ, antes de empezar necesito darte algunas reglas\")\n",
    "    print_slow('1) No entiendo negativos, por ejemplo \"No quiero\" o \"No me siento bien\"')\n",
    "    print_slow('2) Necesito que tus respuestas siempre sean \"(como te sentis),(que buscas)\" para entenderte mejor!')\n",
    "    print_slow('3) A veces mis respuestas no son las mejores, te pido disculpas, mis creadores tuvieron algunos problemas cuando me dise침aron')\n",
    "    print_slow(\"Ahora te pido un minuto para prepararme\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "\n",
    "    # Llamar a la funci칩n principal que maneja la predicci칩n y recomendaciones\n",
    "    main(developer_mode)\n",
    "\n",
    "    # Despedirse del usuario al finalizar\n",
    "    print(\"Nos vemos 游녦\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Hola, mi nombre es ChatLPJ, antes de empezar necesito darte algunas reglas\n",
      "1) No entiendo negativos, por ejemplo \"No quiero\" o \"No me siento bien\"\n",
      "2) Necesito que tus respuestas siempre sean \"(como te sentis),(que buscas)\" para entenderte mejor!\n",
      "3) A veces mis respuestas no son las mejores, te pido disculpas, mis creadores tuvieron algunos problemas cuando me dise침aron\n",
      "Ahora te pido un minuto para prepararme\n",
      "-------------------------------------------------------\n",
      "Accuracy: 0.9996318114874816\n",
      "Confusion Matrix:\n",
      "[[1879    0]\n",
      " [   1  836]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1879\n",
      "           1       1.00      1.00      1.00       837\n",
      "\n",
      "    accuracy                           1.00      2716\n",
      "   macro avg       1.00      1.00      1.00      2716\n",
      "weighted avg       1.00      1.00      1.00      2716\n",
      "\n",
      "Cross-validation scores: [0.99125 0.9925  0.99375 0.99375 0.995  ]\n",
      "Mean cross-validation score: 0.99325\n",
      "Nos vemos 游녦\n"
     ]
    }
   ],
   "source": [
    "chatbot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
