{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "!pip install deep-translator\n",
    "!pip install imbalanced-learn\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install scikit-learn\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Franco-\n",
      "[nltk_data]     SEC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from deep_translator import GoogleTranslator\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "import gdown\n",
    "from IPython.display import display\n",
    "import time\n",
    "import sys\n",
    "from IPython.display import clear_output\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    TextPreprocessor es una clase de preprocesamiento de texto para limpiar datos textuales.\n",
    "    Hereda de BaseEstimator y TransformerMixin de scikit-learn para integrarse en pipelines\n",
    "    y garantizar compatibilidad con la API de scikit-learn.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Conjunto de stopwords en inglés que serán eliminadas del texto\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Limpia el texto de entrada realizando los siguientes pasos:\n",
    "        1. Convierte el texto a minúsculas.\n",
    "        2. Elimina la puntuación.\n",
    "        3. Elimina las stopwords.\n",
    "        \n",
    "        Parámetros:\n",
    "        text (str): Texto de entrada a limpiar.\n",
    "\n",
    "        Retorna:\n",
    "        str: Texto limpio.\n",
    "        \"\"\"\n",
    "        # Convertir el texto a minúsculas\n",
    "        text = text.lower()\n",
    "        # Eliminar puntuación del texto\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        # Eliminar stopwords filtrando palabras en el texto\n",
    "        text = ' '.join(word for word in text.split() if word not in self.stop_words)\n",
    "        return text\n",
    "    \n",
    "    def fit(self, X: pd.Series, y=None) -> \"TextPreprocessor\":\n",
    "        \"\"\"\n",
    "        Método de ajuste para cumplir con la API de scikit-learn.\n",
    "        Este transformador no necesita aprender nada de los datos.\n",
    "        \n",
    "        Parámetros:\n",
    "        X (pd.Series): Serie de pandas con los datos textuales de entrada.\n",
    "        y: Parámetro opcional para compatibilidad; no se utiliza.\n",
    "\n",
    "        Retorna:\n",
    "        TextPreprocessor: Retorna la instancia actual del objeto.\n",
    "        \"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.Series) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Aplica el proceso de limpieza de texto a los datos de entrada.\n",
    "        \n",
    "        Parámetros:\n",
    "        X (pd.Series o str): Datos de entrada a transformar, ya sea una Serie de pandas o una cadena individual.\n",
    "        \n",
    "        Retorna:\n",
    "        pd.Series o str: Datos limpiados, retornados como una Serie de pandas si la entrada fue una Serie,\n",
    "                         o como una cadena individual si la entrada fue una cadena.\n",
    "        \n",
    "        Excepciones:\n",
    "        ValueError: Si el tipo de entrada no es ni una Serie de pandas ni una cadena.\n",
    "        \"\"\"\n",
    "        # Verificar si la entrada es una Serie de pandas\n",
    "        if isinstance(X, pd.Series):\n",
    "            return X.apply(self.clean_text)\n",
    "        # Verificar si la entrada es una cadena individual\n",
    "        elif isinstance(X, str):\n",
    "            return self.clean_text(X)\n",
    "        # Lanzar un error si el tipo de entrada es inválido\n",
    "        else:\n",
    "            raise ValueError(\"La entrada debe ser una cadena o una Serie de pandas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_database() -> None:\n",
    "    \"\"\"\n",
    "    Descarga varios archivos de datos desde Google Drive y los guarda en el directorio 'database'.\n",
    "    Cada archivo se descarga mediante su ID de Google Drive específico y se guarda con un nombre predefinido.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Diccionario que contiene el ID de cada archivo y su ruta de salida correspondiente\n",
    "    files_to_download = {\n",
    "        '1xkN6-OKnp8XOCjcRXFHyFAmKN6GODpji': 'database/bgg_database.csv',\n",
    "        '1b4PUV-SRkUm7A_vLeRZMkA80rsABJjuV': 'database/IMDB-Movie-Data.csv',\n",
    "        '1zJYm3kKzy1HQzta6aCmikTENgPOIrx28': 'database/libros.csv',\n",
    "        '1dfduFDeHbIFoXhNj8FCV7yvuvwKn_rpl': 'database/negative-words.txt',\n",
    "        '17jI37fqKDp9yzgAiwTIE3UmFvkL5XHO1': 'database/positive-words.txt'\n",
    "    }\n",
    "\n",
    "    # Descargar cada archivo usando su ID y guardarlo en la ubicación especificada\n",
    "    for file_id, output_path in files_to_download.items():\n",
    "        # Construir la URL de descarga usando el ID del archivo\n",
    "        download_url = f'https://drive.google.com/uc?id={file_id}'\n",
    "        # Descargar el archivo desde Google Drive y guardarlo en la ruta especificada\n",
    "        gdown.download(download_url, output_path, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_books(developer_mode: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Descarga información de libros populares del sitio Proyecto Gutenberg.\n",
    "    Extrae el título, autor, enlace, resumen y temas de cada libro, y guarda los datos en un archivo CSV.\n",
    "\n",
    "    Parámetros:\n",
    "    developer_mode (bool): Modo desarrollador. Si es True, imprime información detallada de cada libro mientras se procesa.\n",
    "\n",
    "    Salida:\n",
    "    None: La función guarda los datos en un archivo CSV y no retorna ningún valor.\n",
    "    \"\"\"\n",
    "    \n",
    "    # URL de la página de libros más populares\n",
    "    url: str = \"https://www.gutenberg.org/browse/scores/top1000.php#books-last1\"\n",
    "\n",
    "    # Realizar solicitud a la página principal de libros\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Seleccionar enlaces de libros que contienen '/ebooks/' seguido de un número (ID de libro)\n",
    "    libros = soup.select(\"li > a[href^='/ebooks/']\")\n",
    "    datos_libros: List[Dict[str, str]] = []  # Lista para almacenar la información de cada libro\n",
    "\n",
    "    # Procesar cada enlace de libro\n",
    "    for libro in libros:\n",
    "        texto_completo: str = libro.get_text()\n",
    "        enlace_anidado: str = libro['href']\n",
    "        \n",
    "        # Verificar que el enlace tenga un número después de \"/ebooks/\"\n",
    "        if enlace_anidado.split('/ebooks/')[-1].isdigit():\n",
    "            # Separar título y autor; si no hay autor, se asigna \"Desconocido\"\n",
    "            titulo_y_autor: List[str] = texto_completo.split(\" by \", 1)\n",
    "            titulo: str = titulo_y_autor[0].strip()\n",
    "            autor: str = titulo_y_autor[1].strip() if len(titulo_y_autor) > 1 else \"Desconocido\"\n",
    "            \n",
    "            # Construir URL completa para la página del libro\n",
    "            url_libro: str = f\"https://www.gutenberg.org{enlace_anidado}\"\n",
    "            \n",
    "            # Realizar solicitud a la página individual del libro\n",
    "            response_libro = requests.get(url_libro)\n",
    "            response_libro.raise_for_status()\n",
    "            soup_libro = BeautifulSoup(response_libro.text, 'html.parser')\n",
    "            \n",
    "            # Intentar extraer el resumen si está disponible\n",
    "            summary: str = \"\"\n",
    "            summary_row = soup_libro.find('th', text='Summary')\n",
    "            if summary_row:\n",
    "                summary_td = summary_row.find_next(\"td\")\n",
    "                if summary_td:\n",
    "                    summary = summary_td.get_text(strip=True)\n",
    "            \n",
    "            # Extraer temas (subjects) del libro\n",
    "            subjects: List[str] = []\n",
    "            subject_rows = soup_libro.find_all('th', text='Subject')\n",
    "            for subject_row in subject_rows:\n",
    "                subject_td = subject_row.find_next(\"td\")\n",
    "                if subject_td:\n",
    "                    subject_links = subject_td.find_all('a')\n",
    "                    for link in subject_links:\n",
    "                        subjects.append(link.get_text(strip=True))\n",
    "\n",
    "            # Combinar los temas en una sola cadena\n",
    "            subjects_combined: str = \", \".join(subjects)\n",
    "\n",
    "            # Añadir la información del libro a la lista de datos\n",
    "            datos_libros.append({\n",
    "                \"Título\": titulo,\n",
    "                \"Autor\": autor,\n",
    "                \"Enlace\": url_libro,\n",
    "                \"Resumen\": summary,\n",
    "                \"Subjects\": subjects_combined\n",
    "            })\n",
    "            \n",
    "            # Imprimir información detallada en modo desarrollador\n",
    "            if developer_mode:\n",
    "                print(\"Título:\", titulo, \"Autor:\", autor, \"URL:\", url_libro, \"Resumen:\", summary, \"Temas:\", subjects_combined)\n",
    "\n",
    "    # Crear un DataFrame y guardar los datos en un archivo CSV\n",
    "    df_libros = pd.DataFrame(datos_libros)\n",
    "    df_libros.to_csv('libros.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_words(file_path: str) -> set[str]:\n",
    "    \"\"\"\n",
    "    Carga palabras desde un archivo y las retorna como un conjunto de cadenas.\n",
    "    \n",
    "    Parámetros:\n",
    "    file_path (str): La ruta del archivo de texto que contiene una palabra por línea.\n",
    "    \n",
    "    Retorna:\n",
    "    Set[str]: Un conjunto de palabras (strings) obtenidas del archivo, sin espacios en blanco.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Lee todas las líneas del archivo y elimina los espacios en blanco alrededor de cada palabra\n",
    "        words = {line.strip() for line in file.readlines()}\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets_union() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Carga y preprocesa tres conjuntos de datos (películas, juegos de mesa y libros),\n",
    "    limpiando el texto de las columnas relevantes y combinándolos en un solo DataFrame.\n",
    "    \n",
    "    Realiza las siguientes acciones:\n",
    "    1. Procesa y limpia los datos de películas.\n",
    "    2. Procesa y limpia los datos de juegos de mesa.\n",
    "    3. Procesa y limpia los datos de libros.\n",
    "    4. Combina todos los datos en un solo DataFrame.\n",
    "\n",
    "    Retorna:\n",
    "    pd.DataFrame: Un DataFrame combinado que contiene los datos preprocesados.\n",
    "    \"\"\"\n",
    "    # Instanciar el preprocesador de texto\n",
    "    text_preprocessor = TextPreprocessor()\n",
    "\n",
    "    # 1. Procesamiento de Películas\n",
    "    peliculas_dataframe = pd.read_csv('database/IMDB-Movie-Data.csv')\n",
    "    peliculas_dataframe['category'] = 'pelicula'\n",
    "    peliculas_dataframe['text'] = (\n",
    "        peliculas_dataframe['Title'].fillna('').apply(text_preprocessor.transform) + \" \" +\n",
    "        peliculas_dataframe['Genre'].fillna('').apply(text_preprocessor.transform) + \" \" +\n",
    "        peliculas_dataframe['Description'].fillna('').apply(text_preprocessor.transform) + \" \" +\n",
    "        peliculas_dataframe['Director'].fillna('').apply(text_preprocessor.transform) + \" \" +\n",
    "        peliculas_dataframe['Actors'].fillna('').apply(text_preprocessor.transform)\n",
    "    )\n",
    "    peliculas_dataframe.rename(columns={'Title': 'Titulo', 'Director': 'Autor', 'Description': 'Resumen', 'Genre': 'Subjects'}, inplace=True)\n",
    "\n",
    "    # 2. Procesamiento de Juegos de Mesa\n",
    "    juegos_mesa_dataframe = pd.read_csv('database/bgg_database.csv')\n",
    "    juegos_mesa_dataframe['category'] = 'juego'\n",
    "    juegos_mesa_dataframe['text'] = (\n",
    "        juegos_mesa_dataframe['game_name'].fillna('').apply(text_preprocessor.transform) + \" \" +\n",
    "        juegos_mesa_dataframe['description'].fillna('').apply(text_preprocessor.transform) + \" \" +\n",
    "        juegos_mesa_dataframe['designers'].apply(lambda x: ' '.join(eval(x)) if isinstance(x, str) else '').apply(text_preprocessor.transform).fillna('') + \" \" +\n",
    "        juegos_mesa_dataframe['mechanics'].apply(lambda x: ' '.join(eval(x)) if isinstance(x, str) else '').apply(text_preprocessor.transform).fillna('') + \" \" +\n",
    "        juegos_mesa_dataframe['categories'].apply(lambda x: ' '.join(eval(x)) if isinstance(x, str) else '').apply(text_preprocessor.transform).fillna('')\n",
    "    )\n",
    "    juegos_mesa_dataframe.rename(columns={'game_name': 'Titulo','game_href': 'Enlace', 'designers': 'Autor', 'description': 'Resumen', 'mechanics': 'Subjects'}, inplace=True)\n",
    "\n",
    "    # 3. Procesamiento de Libros\n",
    "    libros_dataframe = pd.read_csv('database/libros.csv')\n",
    "    libros_dataframe['category'] = 'libro'\n",
    "    libros_dataframe['text'] = (\n",
    "        libros_dataframe['Título'].fillna('').apply(text_preprocessor.transform) + \" \" +\n",
    "        libros_dataframe['Resumen'].fillna('').apply(text_preprocessor.transform) + \" \" +\n",
    "        libros_dataframe['Subjects'].fillna('').apply(text_preprocessor.transform) + \" \" +\n",
    "        libros_dataframe['Autor'].fillna('').apply(text_preprocessor.transform)\n",
    "    )\n",
    "    libros_dataframe.rename(columns={'Título': 'Titulo', 'Autor': 'Autor', 'Resumen': 'Resumen', 'Subjects': 'Subjects'}, inplace=True)\n",
    "\n",
    "    # Concatenar los DataFrames de películas, juegos de mesa y libros en un solo DataFrame\n",
    "    dataframe_bbdd = pd.concat(\n",
    "        [peliculas_dataframe[['Titulo', 'Autor', 'Resumen', 'Subjects', 'category', 'text']],\n",
    "         juegos_mesa_dataframe[['Titulo', 'Enlace','Autor', 'Resumen', 'Subjects', 'category', 'text']],\n",
    "         libros_dataframe[['Titulo', 'Enlace','Autor', 'Resumen', 'Subjects', 'category', 'text']]\n",
    "        ],\n",
    "        ignore_index=True  # Para reiniciar el índice después de la concatenación\n",
    "    )\n",
    "\n",
    "    return dataframe_bbdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text: str, positive_words: set[str], negative_words: set[str]) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Cuenta las palabras positivas y negativas en un texto dado.\n",
    "\n",
    "    Esta función cuenta cuántas palabras del texto pertenecen al conjunto de\n",
    "    palabras positivas y cuántas al conjunto de palabras negativas.\n",
    "\n",
    "    Parámetros:\n",
    "    - text (str): El texto en el cual se realizarán las búsquedas de palabras.\n",
    "    - positive_words (Set[str]): Un conjunto de palabras consideradas positivas.\n",
    "    - negative_words (Set[str]): Un conjunto de palabras consideradas negativas.\n",
    "\n",
    "    Retorna:\n",
    "    - pd.Series: Una serie de pandas con dos valores, el conteo de palabras positivas\n",
    "      en el texto y el conteo de palabras negativas en el texto, en ese orden.\n",
    "    \"\"\"\n",
    "    # Contar las palabras positivas en el texto\n",
    "    positive_count = sum(1 for word in text.split() if word in positive_words)\n",
    "    \n",
    "    # Contar las palabras negativas en el texto\n",
    "    negative_count = sum(1 for word in text.split() if word in negative_words)\n",
    "    \n",
    "    # Retornar los resultados como una Serie de pandas\n",
    "    return pd.Series([positive_count, negative_count], index=['Positive_Count', 'Negative_Count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_sentiments(developer_mode: bool) -> tuple[LogisticRegression, set[str], set[str]]:\n",
    "    \"\"\"\n",
    "    Entrena un modelo de regresión logística para predecir sentimientos (positivo o negativo) basado en palabras clave positivas y negativas.\n",
    "\n",
    "    La función carga un conjunto de palabras positivas y negativas desde archivos, las convierte en un DataFrame y genera\n",
    "    características basadas en la frecuencia de aparición de palabras positivas y negativas en el texto. Luego entrena un modelo\n",
    "    de regresión logística para clasificar las palabras como \"feliz\" (positivo) o \"triste\" (negativo).\n",
    "\n",
    "    Parámetros:\n",
    "    - developer_mode (bool): Un indicador para mostrar las métricas de evaluación del modelo (accuracy, matriz de confusión, etc.).\n",
    "\n",
    "    Retorna:\n",
    "    - model (LogisticRegression): El modelo entrenado de regresión logística.\n",
    "    - positive_words (Set[str]): El conjunto de palabras positivas cargadas.\n",
    "    - negative_words (Set[str]): El conjunto de palabras negativas cargadas.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cargar palabras positivas y negativas\n",
    "    positive_words = load_words('database/positive-words.txt')\n",
    "    negative_words = load_words('database/negative-words.txt')\n",
    "    \n",
    "    # Crear un conjunto de datos basado en las palabras\n",
    "    data = {\n",
    "        'text': [],\n",
    "        'label': []\n",
    "    }\n",
    "\n",
    "    # Llenar el conjunto de datos con palabras positivas y negativas\n",
    "    for word in positive_words:\n",
    "        data['text'].append(word)\n",
    "        data['label'].append('feliz')  # Etiqueta \"feliz\" para palabras positivas\n",
    "\n",
    "    for word in negative_words:\n",
    "        data['text'].append(word)\n",
    "        data['label'].append('triste')  # Etiqueta \"triste\" para palabras negativas\n",
    "\n",
    "    # Crear un DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Crear características\n",
    "    df[['positive_count', 'negative_count']] = df['text'].apply(lambda x: count_words(x, positive_words, negative_words))\n",
    "\n",
    "    # Convertir etiquetas a números: \"feliz\" -> 1, \"triste\" -> 0\n",
    "    df['label'] = df['label'].map({'feliz': 1, 'triste': 0})\n",
    "\n",
    "    # Separar características y etiquetas\n",
    "    X = df[['positive_count', 'negative_count']]  # Características\n",
    "    y = df['label']  # Etiquetas\n",
    "\n",
    "    # Dividir el conjunto de datos en entrenamiento y prueba (60% entrenamiento, 40% prueba)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "    # Entrenar el modelo de regresión logística\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    if developer_mode:\n",
    "        # Predecir en el conjunto de prueba\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Evaluar el modelo\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        confusion = confusion_matrix(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred)\n",
    "\n",
    "        print(f'Accuracy: {accuracy}')\n",
    "        print('Confusion Matrix:')\n",
    "        print(confusion)\n",
    "        print('Classification Report:')\n",
    "        print(report)\n",
    "    \n",
    "    # Retornar el modelo entrenado y los conjuntos de palabras positivas y negativas\n",
    "    return model, positive_words, negative_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataset: pd.DataFrame, developer_mode: bool) -> any:\n",
    "    \"\"\"\n",
    "    Entrena un modelo de clasificación de texto utilizando regresión logística y TF-IDF para vectorización de texto.\n",
    "    \n",
    "    La función divide el conjunto de datos en entrenamiento y prueba, luego entrena un modelo con regresión logística y realiza \n",
    "    una validación cruzada para evaluar el rendimiento del modelo. Si 'developer_mode' es True, imprime las puntuaciones de la \n",
    "    validación cruzada y la puntuación media.\n",
    "    \n",
    "    Parámetros:\n",
    "    - dataset (pd.DataFrame): Un DataFrame con columnas 'text' (texto) y 'category' (categorías de las clases).\n",
    "    - developer_mode (bool): Un indicador para imprimir las métricas de la validación cruzada si es True.\n",
    "\n",
    "    Retorna:\n",
    "    - model (Pipeline): El modelo entrenado con regresión logística y vectorización TF-IDF.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Separar el dataset en conjunto de entrenamiento y conjunto de prueba (80% - 20%)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset['text'], dataset['category'], test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Crear un pipeline con TF-IDF para la vectorización de texto y regresión logística\n",
    "    model = make_pipeline(\n",
    "        TfidfVectorizer(),  # Vectoriza el texto con TF-IDF\n",
    "        LogisticRegression(C=1.0, max_iter=200, solver='liblinear')  # Modelo de regresión logística\n",
    "    )\n",
    "    \n",
    "    # Entrenar el modelo\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Realizar validación cruzada (5 pliegues)\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5)  # 5-fold cross-validation\n",
    "    \n",
    "    # Si 'developer_mode' es True, imprimir detalles de la validación cruzada\n",
    "    if developer_mode:\n",
    "        print(f\"Cross-validation scores: {cv_scores}\")\n",
    "        print(f\"Mean cross-validation score: {cv_scores.mean()}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_recommender(dataset) -> any:\n",
    "    \"\"\"\n",
    "    Esta función vectoriza los textos del dataset utilizando el algoritmo TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "    para representar los textos como vectores numéricos.\n",
    "\n",
    "    Parámetros:\n",
    "    - dataset (pd.DataFrame): El DataFrame que contiene la columna 'text', que contiene los textos a vectorizar.\n",
    "\n",
    "    Retorna:\n",
    "    - tfidf_matrix (scipy.sparse.csr_matrix): Matriz dispersa de características vectorizadas del texto.\n",
    "    - tfidf_vectorizer (TfidfVectorizer): El objeto del vectorizador que puede usarse para transformar más texto.\n",
    "    \"\"\"\n",
    "    # Crear un objeto TfidfVectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Ajustar y transformar los textos en una matriz TF-IDF\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(dataset['text'])\n",
    "    \n",
    "    return tfidf_matrix, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_entrada_usuario(texto) -> str:\n",
    "    \"\"\"\n",
    "    Procesa la entrada del usuario de la siguiente manera:\n",
    "    1. Traduce el texto de español a inglés.\n",
    "    2. Preprocesa el texto traducido para limpieza (ej. eliminación de stopwords, caracteres no deseados).\n",
    "\n",
    "    Parámetros:\n",
    "    - texto (str): Texto en español ingresado por el usuario.\n",
    "\n",
    "    Retorna:\n",
    "    - texto_procesado (str): Texto limpio y procesado en inglés.\n",
    "    \"\"\"\n",
    "    # Traducir al inglés\n",
    "    texto_traducido = GoogleTranslator(source='es', target='en').translate(texto)\n",
    "\n",
    "    # Preprocesar el texto traducido (limpieza de texto)\n",
    "    texto_procesado = TextPreprocessor().clean_text(texto_traducido)\n",
    "\n",
    "    return texto_procesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_item(prompt, model) -> str:\n",
    "    \"\"\"\n",
    "    Esta función recomienda un artículo basado en el prompt del usuario.\n",
    "    El modelo predictivo se usa para predecir la categoría del texto de entrada.\n",
    "    \n",
    "    Parámetros:\n",
    "    - prompt (str): El texto de entrada del usuario.\n",
    "    - model (modelo entrenado): El modelo que se usará para predecir la categoría.\n",
    "\n",
    "    Retorna:\n",
    "    - category (str): La categoría predicha por el modelo para el prompt ingresado.\n",
    "    \"\"\"\n",
    "    # Predecir la categoría del prompt\n",
    "    category = model.predict([prompt])[0]\n",
    "    \n",
    "    return category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiments(model, positive_words, negative_words, frase, developer_mode) -> str:\n",
    "    \"\"\"\n",
    "    Esta función predice la emoción (feliz o triste) en función de la frase dada.\n",
    "    La predicción se realiza contando las palabras positivas y negativas en la frase procesada \n",
    "    y utilizando un modelo previamente entrenado.\n",
    "\n",
    "    Parámetros:\n",
    "    - model (LogisticRegression o similar): El modelo entrenado para predecir la emoción.\n",
    "    - positive_words (list): Lista de palabras positivas.\n",
    "    - negative_words (list): Lista de palabras negativas.\n",
    "    - frase (str): La entrada de texto a procesar y analizar.\n",
    "    - developer_mode (bool): Indica si se deben imprimir detalles adicionales para depuración.\n",
    "\n",
    "    Retorna:\n",
    "    - emocion (str): La emoción predicha, que puede ser 'feliz' o 'triste'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Procesar la entrada de texto (traducción y limpieza)\n",
    "    entrada_procesada = procesar_entrada_usuario(frase)\n",
    "    \n",
    "    if developer_mode:\n",
    "        print(\"Texto procesado:\", entrada_procesada)\n",
    "\n",
    "    # Contar las palabras positivas y negativas en la entrada procesada\n",
    "    entrada_counts = count_words(entrada_procesada, positive_words, negative_words)\n",
    "\n",
    "    # Asegurarse de que la entrada esté en formato 2D para la predicción\n",
    "    entrada_vectorizada = np.array([entrada_counts]).reshape(1, -1)\n",
    "\n",
    "    # Obtener la predicción del modelo: 1 para 'feliz', 0 para 'triste'\n",
    "    prediccion = model.predict(entrada_vectorizada)[0]\n",
    "\n",
    "    # Determinar la emoción basada en la predicción (1 -> feliz, 0 -> triste)\n",
    "    emocion = 'feliz' if prediccion == 1 else 'triste'\n",
    "    \n",
    "    return emocion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_recomendation(prompt, category, model, dataframe_bbdd) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Esta función genera recomendaciones basadas en un 'prompt' de entrada,\n",
    "    filtrando las recomendaciones según la categoría dada, y luego calculando\n",
    "    la similitud de coseno entre el 'prompt' y los elementos del conjunto de datos.\n",
    "\n",
    "    Parámetros:\n",
    "    - prompt (str): El texto de entrada para la recomendación.\n",
    "    - category (str): La categoría de recomendaciones a considerar.\n",
    "    - model (Pipeline): El modelo entrenado que incluye el vectorizador TF-IDF.\n",
    "    - dataframe_bbdd (DataFrame): El conjunto de datos de recomendaciones, con columnas 'category', 'text', 'Titulo', 'Enlace'.\n",
    "    \n",
    "    Retorna:\n",
    "    - Titulo (str): El título de la recomendación más similar al 'prompt'.\n",
    "    - Enlace (str): El enlace relacionado con la recomendación.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filtrar el dataframe_bbdd para obtener solo las recomendaciones de la categoría dada.\n",
    "    recommendations = dataframe_bbdd[dataframe_bbdd['category'] == category].copy().reset_index(drop=True)\n",
    "\n",
    "    # Si no hay recomendaciones disponibles para la categoría, devolver un mensaje.\n",
    "    if recommendations.empty:\n",
    "        return \"No hay recomendaciones disponibles\", \"\"\n",
    "\n",
    "    # Vectorizar el 'prompt' de recomendación utilizando el vectorizador TF-IDF del modelo.\n",
    "    prompt_vector = model.named_steps['tfidfvectorizer'].transform([prompt])\n",
    "\n",
    "    # Vectorizar el texto de las recomendaciones del dataset usando el vectorizador TF-IDF del modelo.\n",
    "    dataset_vector = model.named_steps['tfidfvectorizer'].transform(recommendations['text'])\n",
    "\n",
    "    # Calcular la similitud de coseno entre el 'prompt' y el texto de las recomendaciones del dataset.\n",
    "    similarity_scores = cosine_similarity(prompt_vector, dataset_vector)\n",
    "\n",
    "    # Obtener el índice del ítem más similar en el dataset basado en la similitud más alta.\n",
    "    most_similar_idx = similarity_scores.argmax()\n",
    "\n",
    "    # Obtener la recomendación correspondiente del dataframe_bbdd utilizando el índice de la similitud más alta.\n",
    "    recommended_item = recommendations.iloc[most_similar_idx]\n",
    "\n",
    "    # Devolver el título y el enlace de la recomendación más similar.\n",
    "    return recommended_item['Titulo'], recommended_item['Enlace']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_user():\n",
    "    \"\"\"\n",
    "    Esta función solicita al usuario una entrada con dos partes: \n",
    "    un sentimiento y una recomendación, separados por una coma. \n",
    "    Si el usuario escribe \"chau\", la función termina la interacción.\n",
    "    \n",
    "    Retorna:\n",
    "    - prompt_sentimiento (str): El texto procesado relacionado con el sentimiento.\n",
    "    - prompt_recomendacion (str): El texto procesado relacionado con la recomendación, si existe.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Solicitar la entrada del usuario\n",
    "    prompt = input(\"¿Cómo estás hoy? ¿Qué quieres hacer? Puedes despedirte diciendo 'Chau': \")\n",
    "    \n",
    "    # Verificar si el usuario quiere terminar la interacción\n",
    "    if prompt.lower() == \"chau\":\n",
    "        return \"exit\", \"exit\"\n",
    "    \n",
    "    # Separar el prompt en sentimientos y recomendaciones usando la coma como delimitador\n",
    "    prompts = prompt.split(\",\")\n",
    "    \n",
    "    # Limpiar y asignar los textos a sus respectivas variables\n",
    "    sentiment_prompt = prompts[0].strip()  # El texto antes de la coma se considera el sentimiento\n",
    "    recommendation_prompt = prompts[1].strip() if len(prompts) > 1 else \"\"  # El texto después de la coma se considera la recomendación, si existe\n",
    "\n",
    "    # Traducir los prompts de español a inglés usando Google Translator\n",
    "    translated_sentiment = GoogleTranslator(source='es', target='en').translate(sentiment_prompt)\n",
    "    translated_recommendation = GoogleTranslator(source='es', target='en').translate(recommendation_prompt)\n",
    "\n",
    "    # Procesar el texto para eliminar caracteres no deseados y hacer limpieza\n",
    "    prompt_sentimiento = TextPreprocessor().clean_text(translated_sentiment)\n",
    "    prompt_recomendacion = TextPreprocessor().clean_text(translated_recommendation)\n",
    "\n",
    "    # Retornar los prompts procesados\n",
    "    return prompt_sentimiento, prompt_recomendacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_slow(text, delay=0.04):\n",
    "    \"\"\"\n",
    "    Imprime un texto lentamente, mostrando un carácter a la vez con un retraso especificado.\n",
    "    \n",
    "    Parámetros:\n",
    "    - text (str): El texto que se imprimirá lentamente.\n",
    "    - delay (float, opcional): El tiempo de espera entre cada carácter en segundos. \n",
    "      Por defecto es 0.04 segundos.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Recorrer cada carácter en el texto\n",
    "    for char in text:\n",
    "        sys.stdout.write(char)  # Escribir el carácter en la salida estándar\n",
    "        sys.stdout.flush()  # Asegurarse de que el carácter se imprima inmediatamente\n",
    "        time.sleep(delay)  # Esperar antes de imprimir el siguiente carácter\n",
    "    \n",
    "    # Imprimir una nueva línea al final\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(developer_mode) -> None:\n",
    "    \"\"\"\n",
    "    Función principal que gestiona el flujo de trabajo de la aplicación.\n",
    "    Verifica la existencia de archivos necesarios, entrena el modelo y\n",
    "    maneja la interacción con el usuario para predecir sentimientos y \n",
    "    hacer recomendaciones.\n",
    "\n",
    "    Parámetros:\n",
    "    - developer_mode (bool): Si está activado, muestra información detallada de depuración.\n",
    "    \"\"\"\n",
    "\n",
    "    # Verificar la existencia de los archivos necesarios\n",
    "    required_files = [\n",
    "        'database/bgg_database.csv',\n",
    "        'database/IMDB-Movie-Data.csv',\n",
    "        'database/libros.csv',\n",
    "        'positive-words.txt',\n",
    "        'negative-words.txt'\n",
    "    ]\n",
    "\n",
    "    # Comprobar si los archivos existen, si no, crear y cargar según corresponda\n",
    "    if not all(os.path.exists(file) for file in required_files):\n",
    "        # Crear el directorio 'database' si no existe\n",
    "        if not os.path.exists('database'):\n",
    "            os.makedirs('database')\n",
    "\n",
    "        # Si el archivo de libros no existe, cargarlo\n",
    "        if not os.path.exists('database/libros.csv'):\n",
    "            load_books()\n",
    "\n",
    "        # Cargar las bases de datos necesarias\n",
    "        load_database()\n",
    "\n",
    "    # Entrenar el modelo de sentimientos\n",
    "    model_sents, positive_words, negative_words = model_sentiments(developer_mode)\n",
    "\n",
    "    # Cargar el dataframe de la base de datos combinada\n",
    "    dataframe_bbdd = load_datasets_union()\n",
    "\n",
    "    # Entrenar el modelo de recomendaciones\n",
    "    recommendation_model = train_model(dataframe_bbdd, developer_mode)\n",
    "\n",
    "    # Ciclo principal de interacción con el usuario\n",
    "    while True:\n",
    "        # Obtener la entrada del usuario (sentimiento y recomendación)\n",
    "        prompt_sentimiento, prompt_recomendacion = input_user()\n",
    "\n",
    "        # Salir si el usuario escribe 'chau'\n",
    "        if prompt_sentimiento == \"exit\":\n",
    "            break\n",
    "\n",
    "        # Predecir sentimiento\n",
    "        sentimiento = predict_sentiments(model_sents, positive_words, negative_words, prompt_sentimiento, developer_mode)\n",
    "\n",
    "        # Predecir la categoría para la recomendación\n",
    "        recommended_category = recommend_item(prompt_recomendacion, recommendation_model)\n",
    "\n",
    "        if developer_mode:\n",
    "            # Imprimir información detallada de la depuración\n",
    "            print(\"Prompt sentimiento:\", prompt_sentimiento)\n",
    "            print(\"Prompt recomendación:\", prompt_recomendacion)\n",
    "            print(\"Categoría recomendada:\", recommended_category)\n",
    "\n",
    "        # Obtener la recomendación final (título y enlace)\n",
    "        recommended_title, recommended_link = predict_recomendation(prompt_recomendacion, recommended_category, recommendation_model, dataframe_bbdd)\n",
    "\n",
    "        # Mostrar los resultados con efectos visuales\n",
    "        print_slow(f\"Entiendo que hoy estás {sentimiento}\")\n",
    "        print_slow(f\"Mi recomendación es: {recommended_title} y el enlace es: {recommended_link} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoy fue un buen dia, quiero ver una pelicula de criminales intergalacticos\n",
    "Tenemos un dia increible con amigos, queremos un juego de mesa de aventuras y ciencia ficcion\n",
    "Hoy fue un dia horrible, quiero leer un libro de biologia\n",
    "Hoy me encuentro mal, quiero ver una pelicula de comedia\n",
    "Hoy discuti con un amigo, me gustaria algo divertido para estar solo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot() -> None:\n",
    "    \"\"\"\n",
    "    Función principal que interactúa con el usuario, configura el modo de desarrollador,\n",
    "    muestra un mensaje de bienvenida con instrucciones, y luego llama a la función principal\n",
    "    que gestiona la predicción de sentimientos y recomendaciones.\n",
    "    \"\"\"\n",
    "\n",
    "    # Activar el modo desarrollador según la respuesta del usuario\n",
    "    developer_mode = False\n",
    "    warnings.filterwarnings(\"ignore\")  # Ignorar advertencias durante la ejecución\n",
    "\n",
    "    # Preguntar al usuario si desea activar el modo desarrollador\n",
    "    if input(\"¿Desea activar el modo desarrollador? (s/n): \\n\") == \"s\":\n",
    "        developer_mode = True\n",
    "\n",
    "    # Limpiar la salida de consola antes de mostrar los mensajes de bienvenida\n",
    "    clear_output(wait=False)\n",
    "\n",
    "    # Mostrar mensajes de bienvenida y reglas de interacción\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print_slow(\"Hola, mi nombre es ChatLPJ, antes de empezar necesito darte algunas reglas\")\n",
    "    print_slow('1) No entiendo negativos, por ejemplo \"No quiero\" o \"No me siento bien\"')\n",
    "    print_slow('2) Necesito que tus respuestas siempre sean \"(como te sentis),(que buscas)\" para entenderte mejor!')\n",
    "    print_slow('3) A veces mis respuestas no son las mejores, te pido disculpas, mis creadores tuvieron algunos problemas cuando me diseñaron')\n",
    "    print_slow(\"Ahora te pido un minuto para prepararme\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "\n",
    "    # Llamar a la función principal que maneja la predicción y recomendaciones\n",
    "    main(developer_mode)\n",
    "\n",
    "    # Despedirse del usuario al finalizar\n",
    "    print(\"Nos vemos 👋\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Hola, mi nombre es ChatLPJ, antes de empezar necesito darte algunas reglas\n",
      "1) No entiendo negativos, por ejemplo \"No quiero\" o \"No me siento bien\"\n",
      "2) Necesito que tus respuestas siempre sean \"(como te sentis),(que buscas)\" para entenderte mejor!\n",
      "3) A veces mis respuestas no son las mejores, te pido disculpas, mis creadores tuvieron algunos problemas cuando me diseñaron\n",
      "Ahora te pido un minuto para prepararme\n",
      "-------------------------------------------------------\n",
      "Accuracy: 0.9996318114874816\n",
      "Confusion Matrix:\n",
      "[[1879    0]\n",
      " [   1  836]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1879\n",
      "           1       1.00      1.00      1.00       837\n",
      "\n",
      "    accuracy                           1.00      2716\n",
      "   macro avg       1.00      1.00      1.00      2716\n",
      "weighted avg       1.00      1.00      1.00      2716\n",
      "\n",
      "Cross-validation scores: [0.99125 0.9925  0.99375 0.99375 0.995  ]\n",
      "Mean cross-validation score: 0.99325\n",
      "Nos vemos 👋\n"
     ]
    }
   ],
   "source": [
    "chatbot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
