{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAWB4aMz2K2N"
      },
      "source": [
        "## Inicializacion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7PHUW_m2K2O",
        "outputId": "027bd7bf-ffb0-45b4-c9b1-e2c8576497cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2024.8.30)\n",
            "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deep-translator\n",
            "Successfully installed deep-translator-1.11.4\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.4)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.5.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gdown\n",
        "!pip install deep-translator\n",
        "!pip install imbalanced-learn\n",
        "!pip install matplotlib\n",
        "!pip install seaborn\n",
        "!pip install scikit-learn\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNPgLMZE2K2P",
        "outputId": "cd201f73-3a7f-4937-c6a0-bb43dee631c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from deep_translator import GoogleTranslator\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import string\n",
        "import re\n",
        "import os\n",
        "import numpy as np\n",
        "import warnings\n",
        "import gdown\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import time\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pzscncGd2K2Q"
      },
      "outputs": [],
      "source": [
        "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.stop_words = set(stopwords.words('english'))  # Cambia a 'spanish' si trabajas con frases en español\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        # Convertir a minúsculas\n",
        "        text = text.lower()\n",
        "        # Eliminar puntuación\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "        # Eliminar stopwords\n",
        "        text = ' '.join([word for word in text.split() if word not in self.stop_words])\n",
        "        return text\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        if isinstance(X, pd.Series):\n",
        "            return X.apply(self.clean_text)\n",
        "        elif isinstance(X, str):\n",
        "            return self.clean_text(X)\n",
        "        else:\n",
        "            raise ValueError(\"Input must be a string or pandas Series.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_database():\n",
        "    file_id = '1xkN6-OKnp8XOCjcRXFHyFAmKN6GODpji'\n",
        "    download_url = f'https://drive.google.com/uc?id={file_id}'\n",
        "    output = 'database/bgg_database.csv'\n",
        "    gdown.download(download_url, output, quiet=True)\n",
        "\n",
        "\n",
        "    file_id = '1b4PUV-SRkUm7A_vLeRZMkA80rsABJjuV'\n",
        "    download_url = f'https://drive.google.com/uc?id={file_id}'\n",
        "    output = 'database/IMDB-Movie-Data.csv'\n",
        "    gdown.download(download_url, output, quiet=True)\n",
        "\n",
        "    file_id = '1zJYm3kKzy1HQzta6aCmikTENgPOIrx28'\n",
        "    download_url = f'https://drive.google.com/uc?id={file_id}'\n",
        "    output = 'database/libros.csv'\n",
        "    gdown.download(download_url, output, quiet=True)\n",
        "\n",
        "    file_id = '1dfduFDeHbIFoXhNj8FCV7yvuvwKn_rpl'\n",
        "    download_url = f'https://drive.google.com/uc?id={file_id}'\n",
        "    output = 'database/negative-words.txt'\n",
        "    gdown.download(download_url, output, quiet=True)\n",
        "\n",
        "    file_id = '17jI37fqKDp9yzgAiwTIE3UmFvkL5XHO1'\n",
        "    download_url = f'https://drive.google.com/uc?id={file_id}'\n",
        "    output = 'database/positive-words.txt'\n",
        "    gdown.download(download_url, output, quiet=True)\n"
      ],
      "metadata": {
        "id": "yQOxEyHO2Ujk"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t12t5IRL2K2Q"
      },
      "source": [
        "## Carga"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LOUT9JEB2K2Q"
      },
      "outputs": [],
      "source": [
        "def load_books(developer_mode):\n",
        "    # URL de la página principal\n",
        "    url = \"https://www.gutenberg.org/browse/scores/top1000.php#books-last1\"\n",
        "\n",
        "    # Hacer la solicitud a la página principal\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Seleccionar solo los enlaces de libros que contienen '/ebooks/' seguido de un número\n",
        "    libros = soup.select(\"li > a[href^='/ebooks/']\")\n",
        "    datos_libros = []\n",
        "\n",
        "    # Extraer datos de cada libro\n",
        "    for libro in libros:\n",
        "        texto_completo = libro.get_text()\n",
        "        enlace_anidado = libro['href']\n",
        "\n",
        "        # Ignorar enlaces que no tienen números después de \"/ebooks/\"\n",
        "        if enlace_anidado.split('/ebooks/')[-1].isdigit():\n",
        "            titulo_y_autor = texto_completo.split(\" by \", 1)\n",
        "            titulo = titulo_y_autor[0].strip()\n",
        "            autor = titulo_y_autor[1].strip() if len(titulo_y_autor) > 1 else \"Desconocido\"\n",
        "\n",
        "            # URL completa del libro\n",
        "            url_libro = f\"https://www.gutenberg.org{enlace_anidado}\"\n",
        "\n",
        "            # Hacer la solicitud a la página del libro\n",
        "            response_libro = requests.get(url_libro)\n",
        "            response_libro.raise_for_status()\n",
        "            soup_libro = BeautifulSoup(response_libro.text, 'html.parser')\n",
        "\n",
        "            # Intentar extraer el summary\n",
        "            summary = \"\"\n",
        "            summary_row = soup_libro.find('th', text='Summary')\n",
        "            if summary_row:\n",
        "                summary_td = summary_row.find_next(\"td\")\n",
        "                if summary_td:\n",
        "                    summary = summary_td.get_text(strip=True)\n",
        "\n",
        "            # Extraer los subjects\n",
        "            subjects = []\n",
        "            subject_rows = soup_libro.find_all('th', text='Subject')\n",
        "            for subject_row in subject_rows:\n",
        "                subject_td = subject_row.find_next(\"td\")\n",
        "                if subject_td:\n",
        "                    subject_links = subject_td.find_all('a')\n",
        "                    for link in subject_links:\n",
        "                        subjects.append(link.get_text(strip=True))\n",
        "\n",
        "            # Unir todos los subjects en una sola cadena\n",
        "            subjects_combined = \", \".join(subjects)\n",
        "\n",
        "            # Agregar datos al listado\n",
        "            datos_libros.append({\n",
        "                \"Título\": titulo,\n",
        "                \"Autor\": autor,\n",
        "                \"Enlace\": url_libro,\n",
        "                \"Resumen\": summary,\n",
        "                \"Subjects\": subjects_combined\n",
        "            })\n",
        "            if developer_mode:\n",
        "                print(\"titulo:\", titulo, \"autor:\", autor, \"url:\", url_libro, \"resumen:\", summary, \"subjects:\", subjects_combined)\n",
        "\n",
        "    # Crear el DataFrame\n",
        "    df_libros = pd.DataFrame(datos_libros)\n",
        "    df_libros.to_csv('database/libros.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "SIhm2ymM2K2R"
      },
      "outputs": [],
      "source": [
        "# Función para cargar palabras desde un archivo\n",
        "def load_words(file_path):\n",
        "    with open(file_path, 'r', encoding='latin-1') as file:  # Cambia a 'utf-16' si es necesario\n",
        "        words = [line.strip() for line in file.readlines()]\n",
        "    return set(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3fpLNQwN2K2R"
      },
      "outputs": [],
      "source": [
        "def load_datasets():\n",
        "    # Instanciar el preprocesador de texto\n",
        "    text_preprocessor = TextPreprocessor()\n",
        "\n",
        "    # 1. Películas\n",
        "    peliculas_dataframe = pd.read_csv('database/IMDB-Movie-Data.csv')\n",
        "    peliculas_dataframe['category'] = 'pelicula'\n",
        "    peliculas_dataframe['text'] = (\n",
        "        peliculas_dataframe['Title'].fillna('').apply(text_preprocessor.transform) + \" \" +\n",
        "        peliculas_dataframe['Genre'].fillna('').apply(text_preprocessor.transform) + \" \" +\n",
        "        peliculas_dataframe['Description'].fillna('').apply(text_preprocessor.transform) + \" \" +\n",
        "        peliculas_dataframe['Director'].fillna('').apply(text_preprocessor.transform) + \" \" +\n",
        "        peliculas_dataframe['Actors'].fillna('').apply(text_preprocessor.transform)\n",
        "    )\n",
        "    peliculas_dataframe.rename(columns={'Title': 'Titulo', 'Director': 'Autor', 'Description': 'Resumen', 'Genre': 'Subjects'}, inplace=True)\n",
        "\n",
        "    # 2. Juegos de Mesa\n",
        "    juegos_mesa_dataframe = pd.read_csv('database/bgg_database.csv')\n",
        "    juegos_mesa_dataframe['category'] = 'juego'\n",
        "    juegos_mesa_dataframe['text'] = (\n",
        "        juegos_mesa_dataframe['game_name'].fillna('').apply(text_preprocessor.transform) + \" \" +\n",
        "        juegos_mesa_dataframe['description'].fillna('').apply(text_preprocessor.transform) + \" \" +\n",
        "        juegos_mesa_dataframe['designers'].apply(lambda x: ' '.join(eval(x)) if isinstance(x, str) else '').apply(text_preprocessor.transform).fillna('') + \" \" +\n",
        "        juegos_mesa_dataframe['mechanics'].apply(lambda x: ' '.join(eval(x)) if isinstance(x, str) else '').apply(text_preprocessor.transform).fillna('') + \" \" +\n",
        "        juegos_mesa_dataframe['categories'].apply(lambda x: ' '.join(eval(x)) if isinstance(x, str) else '').apply(text_preprocessor.transform).fillna('')\n",
        "    )\n",
        "    juegos_mesa_dataframe.rename(columns={'game_name': 'Titulo','game_href': 'Enlace', 'designers': 'Autor', 'description': 'Resumen', 'mechanics': 'Subjects'}, inplace=True)\n",
        "\n",
        "    # 3. Libros\n",
        "    libros_dataframe = pd.read_csv('database/libros.csv')\n",
        "    libros_dataframe['category'] = 'libro'\n",
        "    libros_dataframe['text'] = (\n",
        "        libros_dataframe['Título'].fillna('').apply(text_preprocessor.transform) + \" \" +\n",
        "        libros_dataframe['Resumen'].fillna('').apply(text_preprocessor.transform) + \" \" +\n",
        "        libros_dataframe['Subjects'].fillna('').apply(text_preprocessor.transform) + \" \" +\n",
        "        libros_dataframe['Autor'].fillna('').apply(text_preprocessor.transform)\n",
        "    )\n",
        "    libros_dataframe.rename(columns={'Título': 'Titulo', 'Autor': 'Autor', 'Resumen': 'Resumen', 'Subjects': 'Subjects'}, inplace=True)\n",
        "\n",
        "    # Concatenar los DataFrames manteniendo el original y agregando las nuevas columnas\n",
        "    dataframe_bbdd = pd.concat(\n",
        "        [peliculas_dataframe[['Titulo', 'Autor', 'Resumen', 'Subjects', 'category', 'text']],\n",
        "         juegos_mesa_dataframe[['Titulo', 'Enlace','Autor', 'Resumen', 'Subjects', 'category', 'text']],\n",
        "         libros_dataframe[['Titulo', 'Enlace','Autor', 'Resumen', 'Subjects', 'category', 'text']]\n",
        "        ],\n",
        "        ignore_index=True\n",
        "    )\n",
        "\n",
        "    return dataframe_bbdd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_slow(text, delay=0.04):\n",
        "    for char in text:\n",
        "        sys.stdout.write(char)\n",
        "        sys.stdout.flush()  # Asegura que el texto se imprima inmediatamente\n",
        "        time.sleep(delay)   # Pausa entre cada carácter\n",
        "    print()  # Para agregar una nueva línea al final"
      ],
      "metadata": {
        "id": "NrFAKgGt73nw"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZrfDUbZ2K2R"
      },
      "source": [
        "## Modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8sCdX9Ae2K2S"
      },
      "outputs": [],
      "source": [
        "# Función para contar palabras\n",
        "def count_words(text, positive_words, negative_words):\n",
        "    positive_count = sum(1 for word in text.split() if word in positive_words)\n",
        "    negative_count = sum(1 for word in text.split() if word in negative_words)\n",
        "    return pd.Series([positive_count, negative_count])\n",
        "\n",
        "# Función para preprocesar la entrada del usuario\n",
        "def procesar_entrada_usuario(texto):\n",
        "    # Traducir al inglés\n",
        "    texto_traducido = GoogleTranslator(source='es', target='en').translate(texto)\n",
        "    # Preprocesar el texto traducido\n",
        "    texto_procesado = TextPreprocessor().clean_text(texto_traducido)\n",
        "    return texto_procesado\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mMHoqA452K2S"
      },
      "outputs": [],
      "source": [
        "\n",
        "def model_sentiments(developer_mode):    # Cargar palabras positivas y negativas\n",
        "    positive_words = load_words('database/positive-words.txt')\n",
        "    negative_words = load_words('database/negative-words.txt')# Crear un conjunto de datos basado en las palabras\n",
        "    data = {\n",
        "        'text': [],\n",
        "        'label': []\n",
        "    }\n",
        "\n",
        "    # Llenar el conjunto de datos con palabras positivas y negativas\n",
        "    for word in positive_words:\n",
        "        data['text'].append(word)\n",
        "        data['label'].append('feliz')\n",
        "\n",
        "    for word in negative_words:\n",
        "        data['text'].append(word)\n",
        "        data['label'].append('triste')\n",
        "\n",
        "    # Crear un DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Crear características\n",
        "    df[['positive_count', 'negative_count']] = df['text'].apply(lambda x: count_words(x, positive_words, negative_words))\n",
        "\n",
        "    # Convertir etiquetas a números\n",
        "    df['label'] = df['label'].map({'feliz': 1, 'triste': 0})\n",
        "\n",
        "    # Separar características y etiquetas\n",
        "    X = df[['positive_count', 'negative_count']]\n",
        "    y = df['label']\n",
        "\n",
        "    # Dividir el conjunto de datos en entrenamiento y prueba\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "\n",
        "    # Entrenar el modelo de regresión logística\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    if developer_mode:\n",
        "        # Predecir en el conjunto de prueba\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Evaluar el modelo\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        confusion = confusion_matrix(y_test, y_pred)\n",
        "        report = classification_report(y_test, y_pred)\n",
        "\n",
        "        print(f'Accuracy: {accuracy}')\n",
        "        print('Confusion Matrix:')\n",
        "        print(confusion)\n",
        "        print('Classification Report:')\n",
        "        print(report)\n",
        "\n",
        "    return model, positive_words, negative_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "50nYzsaz2K2S"
      },
      "outputs": [],
      "source": [
        "def train_model(dataset, developer_mode):\n",
        "    # Separar los datos en entrenamiento y prueba\n",
        "    X_train, X_test, y_train, y_test = train_test_split(dataset['text'], dataset['category'], test_size=0.2, random_state=42)\n",
        "\n",
        "    # Crear un pipeline con TF-IDF y regresión logística con hiperparámetros\n",
        "    model = make_pipeline(\n",
        "        TfidfVectorizer(),\n",
        "        LogisticRegression(C=1.0, max_iter=200, solver='liblinear')\n",
        "    )\n",
        "\n",
        "    # Entrenar el modelo\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Validación cruzada\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=5)  # 5-fold cross-validation\n",
        "    if developer_mode:\n",
        "        print(f\"Cross-validation scores: {cv_scores}\")\n",
        "        print(f\"Mean cross-validation score: {cv_scores.mean()}\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "yuF7GbDM2K2S"
      },
      "outputs": [],
      "source": [
        "def recommend_item(prompt, model):\n",
        "    # Predecir la categoría del prompt\n",
        "    category = model.predict([prompt])[0]\n",
        "    return category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Mjg8II1z2K2T"
      },
      "outputs": [],
      "source": [
        "def model_recommender(dataset):\n",
        "    # Vectorizar el texto combinado\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(dataset['text'])\n",
        "\n",
        "    return tfidf_matrix, tfidf_vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6TfwcB6G2K2T"
      },
      "outputs": [],
      "source": [
        "def predict_sentiments(model, positive_words, negative_words, frase, developer_mode):\n",
        "\n",
        "    entrada_procesada = procesar_entrada_usuario(frase)\n",
        "    if developer_mode:\n",
        "        print(\"Texto procesado:\", entrada_procesada)\n",
        "\n",
        "    # Contar palabras en la entrada procesada\n",
        "    entrada_counts = count_words(entrada_procesada, positive_words, negative_words)\n",
        "\n",
        "    # Asegúrate de que la entrada esté en formato 2D\n",
        "    entrada_vectorizada = np.array([entrada_counts]).reshape(1, -1)\n",
        "\n",
        "    # Obtener la predicción de la clase más probable\n",
        "    prediccion = model.predict(entrada_vectorizada)[0]\n",
        "\n",
        "    # Determinar el nombre de la emoción\n",
        "    emocion = 'feliz' if prediccion == 1 else 'triste'\n",
        "\n",
        "    return emocion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "C0PCLB992K2T"
      },
      "outputs": [],
      "source": [
        "def input_user():\n",
        "    prompt_message = \"¿Cómo estás hoy? ¿Qué quieres hacer? Puedes despedirte diciendo 'Chau': \\n\"\n",
        "    print_slow(prompt_message)  # Imprimir el mensaje lentamente\n",
        "    prompt = input()  # Leer la entrada del usuario\n",
        "    if prompt.lower() == \"chau\":\n",
        "        return \"exit\", \"exit\"\n",
        "\n",
        "    # Separar el prompt en sentimientos y recomendaciones por la coma\n",
        "    prompts = prompt.split(\",\")\n",
        "\n",
        "    # Limpiar y asignar los textos\n",
        "    sentiment_prompt = prompts[0].strip()  # Texto antes de la coma\n",
        "    recommendation_prompt = prompts[1].strip() if len(prompts) > 1 else \"\"  # Texto después de la coma\n",
        "\n",
        "    # Traducir los prompts\n",
        "    translated_sentiment = GoogleTranslator(source='es', target='en').translate(sentiment_prompt)\n",
        "    translated_recommendation = GoogleTranslator(source='es', target='en').translate(recommendation_prompt)\n",
        "\n",
        "    # Procesar el texto\n",
        "    prompt_sentimiento = TextPreprocessor().clean_text(translated_sentiment)\n",
        "    prompt_recomendacion = TextPreprocessor().clean_text(translated_recommendation)\n",
        "\n",
        "    return prompt_sentimiento, prompt_recomendacion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "J9bd2ilO2K2T"
      },
      "outputs": [],
      "source": [
        "def predict_recomendation(prompt, category, model, dataframe_bbdd):\n",
        "    # Filtrar el dataframe_bbdd para obtener solo las recomendaciones de la categoría dada\n",
        "    recommendations = dataframe_bbdd[dataframe_bbdd['category'] == category].copy().reset_index(drop=True)\n",
        "\n",
        "    if recommendations.empty:\n",
        "        return \"No hay recomendaciones disponibles\", \"\"\n",
        "\n",
        "    # Vectorizar el prompt de recomendación\n",
        "    prompt_vector = model.named_steps['tfidfvectorizer'].transform([prompt])\n",
        "\n",
        "    # Vectorizar el texto del dataset\n",
        "    dataset_vector = model.named_steps['tfidfvectorizer'].transform(recommendations['text'])\n",
        "\n",
        "    # Calcular la similitud de coseno entre el prompt y el texto del dataset\n",
        "    similarity_scores = cosine_similarity(prompt_vector, dataset_vector)\n",
        "\n",
        "    # Obtener el índice del ítem más similar en el dataset\n",
        "    most_similar_idx = similarity_scores.argmax()\n",
        "\n",
        "    # Obtener la recomendación correspondiente del dataframe_bbdd\n",
        "    recommended_item = recommendations.iloc[most_similar_idx]\n",
        "\n",
        "    return recommended_item['Titulo'], recommended_item['Enlace']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "oCd-vpX52K2T"
      },
      "outputs": [],
      "source": [
        "def main(developer_mode):\n",
        "    # Verificar si el archivo libros.csv existe\n",
        "    if not os.path.exists('database/bgg_database.csv') or not os.path.exists('database/IMDB-Movie-Data.csv') or not os.path.exists('database/libros.csv') or not os.path.exists('positive-words.txt') or not os.path.exists('negative-words.txt'):\n",
        "        if not os.path.exists('database'):\n",
        "            os.makedirs('database')\n",
        "        load_database()\n",
        "    if not os.path.exists('database/libros.csv'):\n",
        "        load_books()\n",
        "\n",
        "    model_sents, positive_words, negative_words = model_sentiments(developer_mode)\n",
        "\n",
        "    dataframe_bbdd = load_datasets()\n",
        "\n",
        "    recommendation_model = train_model(dataframe_bbdd, developer_mode)  # Asegúrate de que este modelo esté entrenado\n",
        "\n",
        "    while True:\n",
        "        prompt_sentimiento, prompt_recomendacion = input_user()\n",
        "\n",
        "        if prompt_sentimiento == \"exit\":\n",
        "            break\n",
        "\n",
        "        sentimiento = predict_sentiments(model_sents, positive_words, negative_words, prompt_sentimiento, developer_mode)\n",
        "\n",
        "        recommended_category = recommend_item(prompt_recomendacion, recommendation_model)  # Predecir la categoría\n",
        "\n",
        "        if developer_mode:\n",
        "            print(\"Prompt sentimiento:\", prompt_sentimiento)\n",
        "            print(\"Prompt recomendación:\", prompt_recomendacion)\n",
        "            print(\"category:\", recommended_category)\n",
        "\n",
        "        recommended_title, recommended_link = predict_recomendation(prompt_recomendacion, recommended_category, recommendation_model, dataframe_bbdd)  # Usar el prompt y el dataset\n",
        "        print(\"\")\n",
        "        print_slow(f\"Entiendo que hoy estas {sentimiento}\")\n",
        "        print_slow(f\"Mi recomendación es: {recommended_title} y el enlace es: {recommended_link} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsWHfYFG2K2T"
      },
      "source": [
        "Hoy fue un buen dia, quiero ver una pelicula de criminales intergalacticos\n",
        "Tenemos un dia increible con amigos, queremos un juego de mesa de aventuras y ciencia ficcion\n",
        "Hoy fue un dia horrible, quiero leer un libro de biologia\n",
        "Hoy me encuentro mal, quiero ver una pelicula de comedia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecj8M-QJ2K2T"
      },
      "source": [
        "## Bot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "Jt2yEg2M2K2T"
      },
      "outputs": [],
      "source": [
        "def chatbot():\n",
        "    developer_mode = False\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    if input(\"¿Desea activar el modo desarrollador? (s/n): \\n\") == \"s\":\n",
        "        developer_mode = True\n",
        "    main(developer_mode)\n",
        "    print(\"Nos vemos 👋\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat"
      ],
      "metadata": {
        "id": "bi3Q6Y3p-zJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZci55QU-3Mj",
        "outputId": "33d9efe6-edb9-4b8c-976f-8a2a35aa2450"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¿Desea activar el modo desarrollador? (s/n): \n",
            "n\n",
            "¿Cómo estás hoy? ¿Qué quieres hacer? Puedes despedirte diciendo 'Chau': \n",
            "\n",
            "Hoy no estoy bien, no quiero una pelicula de comedia\n",
            "\n",
            "Entiendo que hoy estas feliz\n",
            "Mi recomendación es: Disaster Movie y el enlace es: nan \n",
            "\n",
            "¿Cómo estás hoy? ¿Qué quieres hacer? Puedes despedirte diciendo 'Chau': \n",
            "\n",
            "No\n",
            "\n",
            "Entiendo que hoy estas triste\n",
            "Mi recomendación es: Guardians of the Galaxy y el enlace es: nan \n",
            "\n",
            "¿Cómo estás hoy? ¿Qué quieres hacer? Puedes despedirte diciendo 'Chau': \n",
            "\n",
            "Chau\n",
            "Nos vemos 👋\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_txt"
      ],
      "metadata": {
        "id": "wJE3OeO3_2ZR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "NLP",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}